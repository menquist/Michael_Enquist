---
title: "Assignment 3- cross-validation (CV)"
author: "Michael Enquist"
date: "11/30/2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---






```{r,echo=FALSE, warning=FALSE, results='hide'}


#*************************************************************
setwd("~/github/Enquist_Mike/Enquist_Mike/AQM Assignments")
# import example Breast Cancer data set
dat.bc <- na.omit(read.csv("BreastCancer.csv", header = TRUE))


```


```{r}
#------------------ run LOOCV -----------------------------
# initialise prediction vector
isCorrect <- NULL

# perform cv
for(i in 1:nrow(dat.bc))
{
  x.train <- dat.bc[-i,-1] # remove ID
  x.test <- dat.bc[i,-1] # remove ID
  
  mod <- glm(Class ~ ., x.train, family = "binomial")
  pred <- predict(mod, newdata = x.test, type = "response")
  y.pred <- ifelse(pred > 0.5, 1, 0)
  y.true <- x.test$Class
  isCorrect[i] <- ifelse(y.pred == y.true, 1, 0)
}

# misclassification error
sum(isCorrect)/length(isCorrect)

#------------------ run K-fold -----------------------------
error <- NULL
nRuns <- 1000
kSize <- 600

# perform k-fold
for(i in 1:nRuns)
{
  randomSample <- sample(1:nrow(dat.bc), kSize, replace = FALSE)
  x.train <- dat.bc[randomSample, -1] # remove ID
  x.test <- dat.bc[-randomSample, -1] # remove ID
  
  mod <- glm(Class ~ ., x.train, family = "binomial")
  pred <- predict(mod, newdata = x.test, type = "response")
  y.pred <- ifelse(pred > 0.5, 1, 0)
  y.true <- x.test$Class
  isCorrect <- ifelse(y.pred == y.true, 1, 0)
  error[i] <- sum(isCorrect)/length(isCorrect)  
}

# plot error over iterations
plot(cumsum(error)/seq_along(error), type = "l")
```


```{r}
# fit model
mod1 <- glm(Class ~ ., dat.bc[,-1], family = "binomial") # remove ID

#---------------- Bootstrapping ----------------------------
# quantify variance in your model
# set params
error <- NULL
nRuns <- 50
sampleSize <- 50
```


```{r}

for(i in 1:nRuns)
{
  subsetDat <- dplyr::sample_n(dat.bc, sampleSize)
  pred <- predict(mod1, newdata = subsetDat[,-1], type = "response")
  y.pred <- ifelse(pred > 0.5, 1, 0)
  y.true <- subsetDat$Class
  isCorrect <- ifelse(y.pred == y.true, 1, 0)
  error[i] <- sum(isCorrect)/length(isCorrect)
}

mse <- mean(error^2)
```

```{r}

df <- dat.bc
head(df)

#Look up ?factor

df$Class <- factor(df$Class, levels=c(0,1), labels = c("benign","malignant"))




fit.logit <- glm(Class~., data=df, family = binomial())
summary(fit.logit)

prob <- predict(fit.logit, df, type="response")
logit.pred <- factor(prob > .5, levels=c(FALSE, TRUE), labels = c("benign","malignant"))
#By default predict() function predicts the log odds of having a malignant outcome

logit.perf <- table(df$Class, logit.pred, dnn = c("Actual", "Predicted") )
logit.perf


```

##Write maximum 300 words on cross-validation (CV), while considering the following (LaTeX preferred):




These methods try to refit a model of interest to samples formed from the training set. This process is to obtain additional information about the fitted model. Leave-one-out cross-validation (LOOCV) is an estimate of the generalisation performance of a model trained on nâˆ’1 samples of data. Rather than choosing one model, the thing to do is to fit the model to all of the data, and use LOO-CV to provide a slightly conservative estimate of the performance of that model. K-means is used for clustering as a technique for finding similarity groups in a data, called clusters. It attempts to group together by similarity. Bootstrapping is used to estimate a sample distribution by using the information based on a number of resamples from the population. The number of resamples from the sample to estimate the population distribution. The procedure for Bootstrapping is to treat the sample as population, draw x amount of samples sizes of n with replacement to the sample, and try to estimate the sample distribution of the statistic by the bootstrap sample distribution.

Now which method is "better"? The problem is what "better" means. First, these methods is biased for the estimation of the model error (for an infinite amount of future data). Second, if these methods converge to the true model error (if they are not biased), how certain can you be that the correct model error is close? Lastly, which one is "better?", machine learning and data mining use the k-fold cross validation as a preferecen to CV.


The reason we use cross-validation is solve for overfitting.  For example, when using the translink data, we need toidentified our best combination of parameters (in our case time and route) to test the performance. We might want to test Tue and Thu on the 49 busline to ensure that our choices work for those days as well. Also, we have a limited set of data to estimate the unknown parameters. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.





